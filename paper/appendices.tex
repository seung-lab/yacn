\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}

\usepackage[final,nonatbib]{nips_2017}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage[numbers]{natbib}
\usepackage[toc,page]{appendix}
\bibliographystyle{plainnat}
\begin{document}
\begin{appendices}

%-------------------------------------------------------------------------------
% Section A. Baseline Neuronal Boundary Detection
%-------------------------------------------------------------------------------
\section{Baseline Neuronal Boundary Detection}
\label{appendix:baseline}

In this section we describe

\subsection{Architecture}

\begin{figure}[!b]
\centering
\includegraphics[width=1.0\linewidth]{baseline.pdf}

\caption{Architecture for the baseline neuronal boundary detection. Each node represents a layer and the number inside represents the number of feature maps. The layers closer to the top of the diagram have lower resolution than the layers near the bottom. The diagonal arrows represent strided convolutions, while the horizontal arrows represent skip connections.}
\label{fig:architecture}
\end{figure}

\subsection{Dataset}

\subsection{Training}

\subsection{Postprocessing}

\subsection{Inference}


%-------------------------------------------------------------------------------
% Section B. Network Architecture
%-------------------------------------------------------------------------------
\section{Network architectures}
\label{appendix:architecture}

\begin{figure}
\centering
\includegraphics[width=1.0\linewidth]{detector.pdf}
\centering
\includegraphics[width=1.0\linewidth]{corrector2.pdf}

\caption{Architectures for the error detection and error correction modules respectively. Each node represents a layer and the number inside represents the number of feature maps. The layers closer to the top of the diagram have lower resolution than the layers near the bottom. We make savings in computation by minimizing the number of high resolution feature maps. The diagonal arrows represent strided convolutions, while the horizontal arrows represent skip connections.}
\label{fig:architecture}
\end{figure}

Due to the anisotropy of the resolution of the images in our dataset, we design our networks so that the first convolutions are exclusively 2D while later convolutions are 3D. The field of view of a unit in the higher layers is therefore roughly cubic.

To limit the number of parameters in our model, we factorize all 3D convolutions into a 2D convolution followed by a 1d convolution in z. We also use weight sharing between some convolutions at the same height.

\section{Per-object VI score}
\label{appendix:vi}
 Recall that the variation of information between two segmentations may be computed as
\begin{align*}
	VI_{split}&=-\frac 1 {\sum_{i,j} r_{ij}} \sum_{i,j} r_{ij} \log(r_{ij}/p_i)\\
	VI_{merge}&=-\frac 1 {\sum_{i,j} r_{ij}} \sum_{i,j} r_{ij} \log(r_{ij}/q_j)\\
	p_i&=\sum_j r_{ij}\\
	q_j&=\sum_i r_{ij}
\end{align*}
where $r_{ij}$ is the number of voxels in common between the $i^{th}$ segment of the ground truth segmentation and the $j^{th}$ segment of the proposed segmentation \cite{vi}.

We define the split and merge scores for ground truth segment $i$ as
\begin{align*}
	VI_{split}(i) &= -\sum_j r_{ij}/p_i \log(r_{ij}/p_i)\\
	VI_{merge}(i) &= -\sum_j r_{ij}/p_i \log(r_{ij}/q_j)
\end{align*}
Both quantities have units of bits. $VI_{split}(i)$ is zero iff ground truth segment $i$ is contained within a segment in the proposed segmentation, while $VI_{merge}(i)$ is zero iff ground truth segment $i$ is the union of one or more segments in the proposed segmentation. The total score $VI_{split}$ or $VI_{merge}$ is a weighted sum of the per-object scores $VI_{split}(i)$, $VI_{merge}(i)$ respectively.


%-------------------------------------------------------------------------------
% Section B. Training Details
%-------------------------------------------------------------------------------
\section{Training details}
The neural networks were implemented in TensorFlow \cite{tensorflow} and trained using 4 TitanX Pascal GPUs with synchronous gradient descent. We used the Adam optimizer \cite{adam}.   Both networks were trained until the loss on a validation set plateaued. The error detection network trained for 700,000 iterations (approximately one week), while the error-correcting network trained for 1,700,000 iterations (approximately three weeks).


\end{appendices}
\bibliography{bib}
\end{document}
